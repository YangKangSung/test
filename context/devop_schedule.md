나는 devops engineer이고 현재 s/w metric을 자동 측정하는 system을 개발 & 개선 하고 있어
이제 나는 AI를 사용하여 매일 모든 project내에 있는 모든 s/w module의 metric을 자동으로 측정하는 batch job의 schedule에 사용하고 싶어
이 batch job은 매일 오전 9시에 실행되어야 하며, 각 module의 metric을 측정한 후 결과를 데이터베이스에 저장해야 해
측정할 target s/w module들은 각 project마다 100개 정도 있어
현재 batch job은 측정해야할 s/w module들의 목록을 가져오고 이를 순회하며 metric 측정을 triggering하는 방식으로 동작해
이제 AI를 사용하여 이 batch job을 개선하고 싶어

현재 system을 구성하는 요소들은 다음과 같아

1. airflow 1개 - step은 init->build->test->analysis->finish로 구성되어 있음, 반드시 순차적으로 진행됨
    - init: 측정을 위한 jenkins agent를 에약하고 job 실행하도록 전달
    - build: jenkins job에 build cmd를 전달하여 s/w module을 빌드함
    - test: jenkins job에 test cmd를 전달하여 s/w module을 시험함
    - analysis: jenkins job에서 metric 측정하도록 명령전달
    - finish: 분석 결과를 데이터베이스에 저장함
2. jenkins agent 32개
3. 실제 시료에서 시험이 필요한 s/w module들을 위해 시료 시험을 지원하는 test platform 1개
4. test platform과 연결된 각 project의 test 시료들, 이때 각 project에 속한 시험을 지원하는 test 시료는 2~3개 정도 있음

batch job에서 각 s/w module의 metric 측정 workflow path는 다음과 같아

1. airflow dag로 측정 시작 triggering
2. airflow dag에서 jenkins job을 triggering
3. jenkins job에서 build 및 test 후 metric 측정
    - 경우1) test platform과 연결된 test 시료에서 시험이 필요한 s/w module들은 test platform에 연결된 test 시료에서 시험을 진행함
    - 경우2) 시료가 필요없을 때는 test 시료 없이 jenkins agent에서 시험을 진행함
4. jenkins job에서 metric 측정 결과를 airflow dag에 전달
5. airflow dag에서 metric 측정 결과를 데이터베이스에 저장

이제 AI를 사용하여 이 batch job을 개선하고 싶어
개선을 위한 목적은 당연히 batch job의 시간 단축이냐
제한 적인 자원인 아래 사항을 고려하여 모든 s/w module의 schedule queue를 최적화하여 batch job의 시간을 단축하고 싶어

고려해야할 제한 자원 및 제약 사항은 다음과 같아

1. jenkins agent는 32개가 있으며, 각 agent는 동시에 1개의 job만 수행할 수 있음
2. test platform은 1개이며, 동시에 platform에 연결된 여러개의 test 시료에서 시험을 진행할 수 있음
3. 각 project에 속한 시험을 지원하는 test 시료는 2~3개 정도 있음
4. 각 s/w module들의 build 및 test 그리고 측정시간은 모두 다름

위와 같을때 AI를 사용하여 batch job을 개선하기 위한 방법은 무엇인지 제안해줘
강화학습 것을 사용하면 좋을거라고 주변사람들이 추천해 주는데 이걸 사용하여 batch job을 개선할 수 있는지 궁금하고,
강화학습을 사용한다면 어떤 방법으로 batch job을 개선할 수 있는지 구체적으로 설명해줘
강화학습 외에 다른 방법이 있다면 그것도 설명해줘

=================================================================================

훌륭한 시스템 설명 고마워! 강화학습 기반 스케줄 최적화에 대해 구체적인 제안을 드리기 전에, 아래 사항들만 더 알려줄 수 있을까?

1. 각 s/w module에 대한 build/test/analysis 예상 소요 시간은 어떻게 확보되나요? (예: 사전 측정된 평균 값, 예측 모델 등)
   -> influxdb에 airflow의 단계별 소요시간을 기록하고 있어

2. test platform에서 시험이 필요한 s/w module은 전체 중 어느 정도 비율인가요?
   -> 전체 100개 중 약 10% 정도가 test platform에서 시험이 필요해

3. AI를 적용한 개선안을 배포하는 데 있어서 실험 환경이 따로 존재하나요, 아니면 바로 운영에 반영해야 하나요?
   -> 운영에 반영해야 해

4. 강화학습을 적용할 경우, 에피소드 단위로 학습하는 시뮬레이션 환경 구성이 가능한가요?
   -> 에피소드 단위로 학습하는 시뮬레이션 환경은 구성할 수 없어 바로 운영에 반영해서 성능 개선을 원해

5. 성능 목표나 제약 조건이 추가로 있나요? 예: 최대 처리 시간 제한, 특정 프로젝트 우선순위 등
   -> 최대 처리 시간 제한은 없고, 기존 무작위 순서로된 측정시간보다는 점점 짧아졌으면 좋겠어.

이 정보를 바탕으로 강화학습이나 다른 AI 기반 최적화 기법 중 어떤 것이 적합한지 정확히 제안드릴 수 있을 것 같아요!

================================================================================

좋아, AI 특히 강화학습을 포함해 이 batch job을 어떻게 개선할 수 있을지 구체적으로 알아볼게. 제약된 자원(Jenkins agent, test platform 등)과 다양한 module 특성을 고려한 스케줄 최적화를 목표로 해서, 강화학습 기반의 방법 외에도 사용할 수 있는 알고리즘적 대안들까지 정리해서 제안할게.

조금만 기다려줘!

# 배치 작업 스케줄링 최적화: 강화학습 vs 휴리스틱 vs 진화 알고리즘 비교

## 문제 상황 및 최적화 목표

현재 운영 중인 일일 배치 작업(약 100개 모듈)의 총 실행 시간을 줄이기 위해 **스케줄링 최적화**가 필요합니다. 각 모듈은 Airflow DAG 형태로 5단계(`init -> build -> test -> analysis -> finish`)를 순차 수행하며, 병렬 처리는 Jenkins 에이전트(동시에 최대 32개)로 이루어집니다. 이때 모듈의 10% 정도는 **시험 장비(시료)**가 필요한 `test` 단계가 있어 해당 모듈들은 추가적인 자원 제약(프로젝트별 2~3개의 시료, 동시 시험 가능 수 제한)을 갖습니다. 현재는 모듈 실행 순서가 무작위이므로, 작업 **총 완료시간(makespan)** 단축을 위해 **인공지능 기반 최적 스케줄링**을 도입하고자 합니다. 아래에서는 **강화학습(RL)**을 통한 접근 가능성 및 구체적인 방법, 그리고 이를 대체하거나 보완할 수 있는 **실용적 알고리즘들**(휴리스틱, 진화적 알고리즘, 그래프/제약 기반 방법)을 비교하여 설명합니다.

## 강화학습을 이용한 스케줄 최적화

**강화학습(RL)**은 스케줄링을 **순차적인 의사결정 문제**로 모델링하여, 매 시점마다 어떤 작업을 실행할지 정책을 학습시킬 수 있습니다. 예를 들어, **상태**는 각 모듈의 진행 단계, 사용 가능한 Jenkins 에이전트 수, 시험 시료의 사용 현황 등을 포함하고, **행동**은 다음에 실행할 모듈(또는 할당 방법)을 선택하는 것으로 정의할 수 있습니다. 에이전트는 모든 작업이 완료될 때 얻는 **보상**(예: -총완료시간)을 최대화하도록 학습합니다.

강화학습을 적용한다면 **Off-policy 딥 RL 알고리즘**(예: **Deep Q-Network** 기반 스케줄러) 등이 적절해 보입니다. 이러한 방법은 과거 데이터나 시뮬레이션을 활용해 학습할 수 있어 **실환경에서의 실시간 학습 부담을 줄여**줍니다. 실제로 병렬 기계 스케줄링 문제에 **이중 딥 Q-러닝(Double DQN)**을 적용하여 기존 방법보다 낮은 지연을 달성한 연구도 있습니다 ([Deep Reinforcement Learning-Based Scheduler on Parallel Dedicated Machine Scheduling Problem towards Minimizing Total Tardiness](https://www.mdpi.com/2071-1050/15/4/2920#:~:text=match%20at%20L1075%20From%20this,are%20better%20than%20existing%20methods)). 특히 최근에는 **오프라인 RL** 기법으로 **그래프 신경망**으로 상태를 표현하여 복잡한 제약의 Job-Shop 스케줄링에서 SOTA 수준 성능을 보이는 사례도 보고되었습니다 ([[2410.15714] Offline reinforcement learning for job-shop scheduling problems](https://arxiv.org/abs/2410.15714#:~:text=issues%20and%20neglect%20of%20the,art%20techniques)). 이런 접근은 우리 문제의 DAG 구조와 자원 제약을 표현하기에 적합합니다.

**학습 방법 측면**에서는, 운영 환경에서 에피소드 기반의 대량 시뮬레이션이 어렵기 때문에 **실제 기록된 배치 실행 데이터**로 RL 정책을 학습시키는 방안을 고려합니다. 이미 InfluxDB에 각 단계 소요시간 데이터가 있으므로 이를 활용해 환경 모델을 구성하거나 과거 스케줄링 결과를 **오프라인 RL**로 학습할 수 있습니다. 오프라인 데이터를 활용하는 **비모수(off-policy)** 학습 알고리즘을 쓰면 실제 환경을 방해하지 않고 정책을 훈련할 수 있습니다. 훈련된 정책은 배치 작업 시작 전 스케줄 결정을 실시간으로 내리도록 적용하고, 운영 중 수집되는 새로운 데이터로 주기적으로 재훈련하거나 미세조정하여 점진적으로 향상된 성능을 기대할 수 있습니다.

**강화학습의 장점**은 복잡한 조건에서도 **학습을 통해 점진적으로 최적에 가까운 전략**을 찾을 수 있다는 것입니다. 규칙 기반으로 찾기 어려운 자원 병목이나 순서 최적화를 자동으로 학습하며, 잘 훈련된 RL 에이전트는 휴리스틱보다 **더 낮은 총완료시간**을 달성할 잠재력이 있습니다 ([Deep Reinforcement Learning-Based Scheduler on Parallel Dedicated Machine Scheduling Problem towards Minimizing Total Tardiness](https://www.mdpi.com/2071-1050/15/4/2920#:~:text=match%20at%20L1075%20From%20this,are%20better%20than%20existing%20methods)). 또한 환경 변화에 적응하여 정책을 조정할 수 있으므로 **동적인 스케줄링에 대한 적응성**도 장점입니다.

그러나 **한계 및 구현 난이도**도 높습니다. 일반적인 딥 RL은 **학습 속도가 느리고 많은 시행착오**를 거쳐야 하므로 ([[2410.15714] Offline reinforcement learning for job-shop scheduling problems](https://arxiv.org/abs/2410.15714#:~:text=crucial%20for%20applications%20like%20routing,a%20heterogeneous%20graph%20and%20the)), 충분히 좋은 정책을 얻기까지 시간이 걸릴 수 있습니다. 특히 초기 학습 단계에서 성능이 기존 랜덤 스케줄링보다 나쁠 수도 있어 **운영 초기의 리스크**가 존재합니다. 또한 환경 상태공간과 행동공간이 크기 때문에 신중한 **상태 표현과 보상설계**가 필요합니다. 구현 난이도 역시 높아서, Airflow-Jenkins 시스템과 RL 모듈을 연동하고 학습 인프라를 구축해야 하므로 **개발과 검증에 상당한 시간**이 소요될 것입니다. 요약하면, RL은 **이론적으로 강력**하지만 **실무 적용 난이도**가 높고, 즉각적인 효과보다는 **중장기적인 최적화 전략**으로 적합합니다.

## 휴리스틱 기반 스케줄링

**휴리스틱(heuristic)** 접근은 사람의 전문지식이나 간단한 규칙에 기반하여 작업 순서를 결정하는 방법입니다. 예를 들어, **Shortest Processing Time (SPT)** 규칙으로 예상 실행시간이 가장 짧은 작업부터 실행하거나, **Longest Processing Time (LPT)**으로 긴 작업을 먼저 배치하여 뒤쪽 병목을 줄이는 방식 등이 있습니다. 또는 현재 시스템에 특화된 규칙으로, **시험 장비가 필요한 작업을 분산**시켜 한꺼번에 몰리지 않도록 하거나, **남은 처리시간이 긴 작업**을 우선하여 총 소요시간을 줄이는 커스텀 규칙을 만들 수도 있습니다. 이러한 휴리스틱은 구현이 매우 간단하며, 현재 랜덤 실행 대비 **즉각적인 개선**을 가져올 수 있습니다.

**장점:** 휴리스틱 방법은 **이해와 구현이 쉽고 안정적**입니다. 복잡한 모델 학습 없이도 적용 가능하여 **운영 환경에 바로 투입**할 수 있습니다. 또한 계산 부하가 낮아 **실시간으로도 동작**할 수 있고, 결과가 예측 가능하여 시스템 운영자가 신뢰하기 쉽습니다. 역사적으로도 많은 현실 세계 스케줄링 문제는 전통적인 규칙 기반 또는 우선순위 기반 알고리즘으로 효율적으로 해결되어 왔습니다 ([Why some mathematicians are against meta-heuristic and heuristic algorithms? | ResearchGate](https://www.researchgate.net/post/Why-some-mathematicians-are-against-meta-heuristic-and-heuristic-algorithms#:~:text=I%20have%20found%20that%20some,don%27t%20posses%20a%20convergence%20theory)). 잘 설계된 규칙 조합을 쓰면 비교적 **높은 품질의 해법**을 빠르게 얻을 수 있습니다.

**단점:** 휴리스틱은 대개 **글로벌 최적해를 보장하지 못하고**, 규칙이 특정 상황에 편중되면 **일부 시나리오에서 비효율**을 보일 수 있습니다. 예를 들어 SPT 규칙은 평균적 성능은 좋지만 작업들 간 편차가 큰 경우 최적이 아닐 수 있습니다. 또한 규칙 기반 접근은 **문제에 대한 사전 지식과 튜닝**이 필요하며, 환경이 변화하면 규칙을 수정해야 하는 **유연성 한계**가 있습니다. 복잡한 제약(예: 시험 시료 병목과 Jenkins 에이전트 병목의 상호작용)을 고려하는 다중 규칙을 수동 설계하기도 어렵습니다. 그럼에도 불구하고 구현 난이도가 매우 낮고 **안정적으로 운영**할 수 있으므로, **단기적인 솔루션**이나 RL 도입 전 **베이스라인**으로 유용합니다.

## 진화 알고리즘 기반 최적화

**진화적 알고리즘**(evolutionary algorithm)은 유전 알고리즘(GA), **시뮬레이티드 어닐링(SA)**, **탐욕적 랜덤 적응 탐색(GRASP)** 등 자연 현상이나 무작위 탐색에 기반한 **메타휴리스틱** 기법을 사용합니다. 이 접근에서는 **스케줄링 해(작업 순서)**를 하나의 **개체(염색체)**로 표현하고, **적합도 함수**를 총 실행시간(또는 지연 시간 등)으로 정의합니다. GA의 경우 초기 무작위 순서 세트에서 시작해 **유전자 교배**와 **돌연변이**로 새로운 작업 순서를 생성하고, **선택** 과정을 통해 점차 더 나은 스케줄을 진화시킵니다. 이렇게 하면 **해 공간 전체를 전역적으로 탐색**하여 우수한 솔루션을 발견할 수 있습니다 ([What is a Genetic Algorithm in Manufacturing - Eyelit Technologies](https://eyelit.ai/what-is-genetic-algorithm/#:~:text=1,particularly%20useful%20for%20solving%20real)). SA나 기타 메타휴리스틱도 다양한 해를 탐색하며 점진적으로 개선하는 원리는 유사합니다.

**장점:** 진화 알고리즘은 **전역 최적 탐색 능력**이 있어 지역해에 머무를 가능성이 적고, 문제 특성에 따라 유연하게 적용 가능합니다. 여러 제약 조건도 **페널티를 주는 방식으로 쉽게 통합**할 수 있어, 시험 시료 제한이나 병행 수행 제약 등을 적절히 포함한 **피트니스 함수**를 설계하면 현실적인 제약을 만족하는 해를 찾습니다. 또한 병렬로 여러 해를 평가하므로 **병렬 계산**을 통해 탐색 속도를 높일 수도 있습니다 ([What is a Genetic Algorithm in Manufacturing - Eyelit Technologies](https://eyelit.ai/what-is-genetic-algorithm/#:~:text=1,particularly%20useful%20for%20solving%20real)). 충분한 탐색 시간을 주면, GA 등의 메타휴리스틱은 **휴리스틱보다 개선된 스케줄**(총완료시간 단축)을 도출할 확률이 높습니다. 예를 들어, GA를 수 분~수십 분간 실행하여 현재 100개 모듈에 대한 다음날 아침 스케줄을 전날 미리 계산해놓고 적용하는 식으로 운용할 수 있습니다.

**단점:** 메타휴리스틱은 **계산자원과 시간이 많이 필요**할 수 있습니다. 해 평가를 위한 시뮬레이션을 반복하므로, 작업 100개의 순열 공간이 매우 큰 점을 고려하면 **실시간 활용은 어렵고** 적어도 수분~수십분의 배치 연산 시간이 필요합니다 ([What is a Genetic Algorithm in Manufacturing - Eyelit Technologies](https://eyelit.ai/what-is-genetic-algorithm/#:~:text=1,operators%20and%20setting%20appropriate%20parameters)). 또한 **매개변수 튜닝**(GA의 인구 크기, 세대 수, 돌연변이 확률 등)이 결과 품질에 영향을 크게 미쳐, 최적의 설정을 찾기까지 시행착오가 필요합니다 ([Why some mathematicians are against meta-heuristic and heuristic algorithms? | ResearchGate](https://www.researchgate.net/post/Why-some-mathematicians-are-against-meta-heuristic-and-heuristic-algorithms#:~:text=I%20have%20found%20that%20some,don%27t%20posses%20a%20convergence%20theory)). 운 나쁘게 **조기 수렴**해버리면 전역 최적해를 찾지 못하고 평범한 해에 멈출 위험도 있습니다(이를 완화하려면 탐색 과정을 다양화해야 함). 구현 난이도는 RL보다는 낮지만 여전히 **커스텀 시뮬레이터** 작성, 알고리즘 파라미터 조절 등 **기술적 노력**이 요구됩니다. 성능 면에서는 충분한 시간을 줄 경우 **최적해에 매우 가까운 일정**을 기대할 수 있지만, 시간 제약 하에서는 휴리스틱+조정 수준의 해에 머물 가능성도 있습니다. 요약하면, 진화 알고리즘은 **중간 정도의 개발 난이도**로 **높은 최적화 성능**을 노릴 수 있는 접근이지만, **계산 비용**과 **튜닝 부담**을 고려해야 합니다.

## 그래프/제약 기반 스케줄링 알고리즘

**그래프 기반** 또는 **수리모델 기반** 접근은 스케줄링 문제를 **명시적인 최적화 문제**로 모델링하여 **정형화된 알고리즘**으로 푸는 방법입니다. 예를 들어 각 작업 단계와 자원 제약을 하나의 **그래프**(노드: 작업 단계, 에지: 순서/자원 제약)로 표현하고, **토폴로지 정렬**이나 **임계경로법(CPM)**을 응용하여 가능한 한 빨리 모든 작업을 완료하는 일정을 구성할 수 있습니다. 또는, 스케줄링을 **혼합정수계획법(MILP)**이나 **제약충족 프로그래밍(CP)** 문제로 수식화하여 **최적해 계산**을 시도할 수도 있습니다. Google OR-Tools 등의 라이브러리를 활용하면 병렬 기계 및 순차 공정이 있는 스케줄링을 제약으로 모델링해 해를 구할 수 있습니다.

**장점:** 이 접근법은 문제를 정확하게 수식화하기 때문에 **제약 조건을 엄밀히 반영**하고 **이론적으로 최적에 가까운 해**를 얻을 수 있습니다. 작은 규모 문제의 경우 **전역 최적해**를 구할 수 있으며, 제약이 추가되거나 변경되어도 모델만 수정하면 되므로 **유지보수**가 논리적으로 명확합니다. 또한 **시프트병목(shifting bottleneck)** heuristic 등 그래프 기반 알고리즘은 많은 Job-shop 스케줄링 문제에서 휴리스틱보다 우수한 결과를 보여온 바 있습니다. 만약 문제 규모가 허용하는 한도 내라면, 수리적 최적화 솔버를 통해 **사전 계산된 최적 스케줄**을 얻어 운영에 활용할 수도 있습니다.

**단점:** 정확해를 찾는 수리모델은 **연산 복잡도**가 매우 높아, 작업 수가 많으면 **계산 시간이 기하급수적으로 증가**합니다. 예를 들어 한 연구에서는 10개 작업 이하 소규모 인스턴스에 대해서만 MILP로 수 분 내 최적해를 구할 수 있었다고 보고하고 있습니다 ([A Mixed-Integer Linear Programming Model for Addressing Efficient Flexible Flow Shop Scheduling Problem with Automatic Guided Vehicles Consideration](https://www.mdpi.com/2076-3417/15/6/3133#:~:text=solver,between%20AGV%20quantity%20and%20makespan)). 우리 경우 100개 작업에 32개 기계, 추가 자원제약까지 있으므로 완전 최적화는 현실적으로 **시간 내 풀리지 않을 가능성**이 높습니다. 제약충족기법(CP)은 경험적으로 MILP보다 큰 문제를 풀 수 있지만, 최적성을 보장하려면 역시 시간이 많이 필요하고, 제한된 시간 안에 해를 얻으면 그것이 최적 보장은 아니게 됩니다. **그래프 기반 휴리스틱**(예: 시프트병목 알고리즘)은 구현이 복잡하고 문제별 커스터마이징이 필요하여, 현업에서 직접 개발·적용하기 어렵습니다. 수리모델 접근은 **전문 지식과 튜닝**이 요구되고, 솔버를 사용하더라도 **통합 및 검증**에 노력이 필요합니다. 따라서 실시간 운영보다는 **한정된 규모에서의 의사결정 지원** 용도로 적합하며, 대규모 문제에는 종종 휴리스틱이나 메타휴리스틱의 보조 도구로 활용됩니다.

## 접근 방법 비교 및 적용 고려사항

아래 표는 제시된 여러 접근 방식의 특징을 요약한 것입니다. 각 방법의 **장단점**, **운영 적용 난이도**, **성능 기대치**를 비교하였으므로, 현재 환경에 맞는 최적의 방안을 선택하는 데 참고할 수 있습니다.

| **접근 방식**                                              | **장점**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | **단점**                                                                                                                                                                                                                                                                                                                                                                                   | **적용 난이도**                                                                                                                                                                                                                                  | **성능 기대치**                                                      |
| ---------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| **강화학습** <br>(예: DQN 기반 스케줄러)                   | - 데이터로부터 최적 전략 **학습**하여 사람보다 나은 스케줄 가능<br>- **환경 변화에 적응**하며 지속 개선<br>- 복잡한 자원 제약도 모델에 통합 가능 ([[2410.15714] Offline reinforcement learning for job-shop scheduling problems](https://arxiv.org/abs/2410.15714#:~:text=issues%20and%20neglect%20of%20the,art%20techniques)) ([Deep Reinforcement Learning-Based Scheduler on Parallel Dedicated Machine Scheduling Problem towards Minimizing Total Tardiness](https://www.mdpi.com/2071-1050/15/4/2920#:~:text=match%20at%20L1075%20From%20this,are%20better%20than%20existing%20methods)) | - **훈련에 많은 에피소드** 필요 (학습 **속도 느림**) ([[2410.15714] Offline reinforcement learning for job-shop scheduling problems](https://arxiv.org/abs/2410.15714#:~:text=crucial%20for%20applications%20like%20routing,a%20heterogeneous%20graph%20and%20the))<br>- 초기 성능 불확실 (학습 실패 시 기존보다 악화 위험)<br>- 구현 복잡 (AI 인프라, 상태·보상 설계 필요)                | 매우 높음 (전문지식 요구, 개발/검증 시간 큼)                                                                                                                                                                                                     | 높음 (잘 학습되면 **최적해 근접** <br>**→** 현재보다 크게 단축 가능) |
| **휴리스틱** <br>(규칙 기반 우선순위)                      | - 매우 **간단**하고 즉시 구현 가능<br>- 검증된 경험 기반 **안정적** 방식<br>- 실시간 의사결정에 **가벼움**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | - 전역 최적화 어려움 (규칙 한계)<br>- 상황 변화에 규칙 **재조정** 필요<br>- 복잡한 상호 제약 반영 어려움                                                                                                                                                                                                                                                                                   | 매우 낮음 (당장 적용 가능)                                                                                                                                                                                                                       | 중간 (랜덤 대비 **확실한 개선**, <br>**하지만** 최적 대비 갭 존재)   |
| **진화 알고리즘** <br>(예: 유전 알고리즘)                  | - **전역 탐색**으로 양질의 해 발견<br>- 제약을 유연하게 **포함** 가능<br>- 기존 방법보다 **낮은 makespan** 달성 기대                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | - **연산 비용** 높음 (다수 해 평가)<br>- 매개변수 **튜닝** 중요 (노하우 필요) ([Why some mathematicians are against meta-heuristic and heuristic algorithms?                                                                                                                                                                                                                               | ResearchGate](https://www.researchgate.net/post/Why-some-mathematicians-are-against-meta-heuristic-and-heuristic-algorithms#:~:text=I%20have%20found%20that%20some,don%27t%20posses%20a%20convergence%20theory))<br>- 수렴 운용에 따라 성능 변동 | 중간 (라이브러리 활용 가능하나<br>도메인별 튜닝 필요)                | 높음 (충분한 시간 탐색 시 <br>**우수한 해** 도출, 시간 부족시 **편차**) |
| **그래프/수리 모델** <br>(예: CP/MILP 솔버, 특화 알고리즘) | - **수리적 최적해** 추구 (해가 최적에 매우 근접)<br>- **제약 조건 완벽 반영** 가능<br>- 솔버 이용 시 구현 **명확**                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | - 대규모 문제에 **계산 한계** (NP-난해) ([A Mixed-Integer Linear Programming Model for Addressing Efficient Flexible Flow Shop Scheduling Problem with Automatic Guided Vehicles Consideration](https://www.mdpi.com/2076-3417/15/6/3133#:~:text=solver,between%20AGV%20quantity%20and%20makespan))<br>- 전문 지식 필요 (모델링 난이도)<br>- 해법 시간 예측 어려움 (최악의 경우 종료 못함) | 높음 (문제 모델링 및 솔버 튜닝 필요)                                                                                                                                                                                                             | 매우 높음 (작은 문제 **최적**,<br>큰 문제는 제한시간 내 **근사해**)  |

**종합 권고:** 현재 운영 환경에 즉시 효과를 내고 싶다면 **휴리스틱 방식**이나 간단한 **메타휴리스틱(진화 알고리즘)**을 먼저 적용하는 것이 현실적입니다. 예를 들어, 예상 소요시간 기반으로 모듈을 정렬하거나, 시험 시료가 필요한 작업을 미리 착수하게 하는 규칙을 도입해 쉽게 **몇십% 수준의 개선**을 얻을 수 있습니다. 이후 추가로 최적화를 원한다면, **유전 알고리즘** 등을 이용해 휴리스틱 결과를 개선하거나, RL 도입을 병행 준비할 수 있습니다. **강화학습**은 장기적으로 가장 강력한 잠재력을 지니나, 초기 투자와 검증이 필요하므로 **오프라인 학습환경 구축**과 **모델 검증**을 충분히 거쳐 점진적으로 도입하는 전략을 권장합니다. 마지막으로, **제약 기반 솔버**는 연구나 검증용으로 **이론상의 최적 기준**을 알아보는 데 활용하고, 얻어진 통찰을 휴리스틱/RL에 반영하는 보조 수단으로 사용하는 것이 바람직합니다. 이러한 단계적 접근을 통해 리스크를 관리하면서도 운영 배치 작업의 총 실행 시간을 크게 단축할 수 있을 것으로 기대됩니다.

좋아, 지금까지 네가 공유한 문제 상황과 요구사항을 바탕으로 정리된 문서를 작성해볼게. 이 문서는 프로젝트 개요, 시스템 구성, 제약 조건, 개선 목표, AI 적용 가능성 및 비교 분석 등을 포함할 거야. 완료되면 언제든 수정하거나 확장할 수 있어.

# AI 기반 S/W 모듈 메트릭 측정 배치 작업 최적화 기획서

## 프로젝트 개요

현재 당사는 Airflow 기반으로 **소프트웨어 모듈 메트릭 측정 배치 작업**을 운영하고 있습니다. 본 프로젝트의 목표는 이 배치 파이프라인의 **스케줄링을 AI 기술로 최적화**하여 전체 작업 완료 시간(전체 실행시간, _makespan_)을 최소화하고 자원 활용 효율을 높이는 것입니다. 기존에는 작업들이 거의 **무작위 순서**로 실행되어 비효율이 발생하고 있으며, 모듈과 작업량이 많아지면서 정해진 시간 내 모든 메트릭 수집을 마치는 데 어려움이 있습니다. DevOps 자동화와 AIOps 흐름에 발맞춰, AI를 활용한 스마트 스케줄링으로 **CI/CD 파이프라인의 성능 개선**과 **자원 최적 활용**을 이루고자 합니다. 이 문서는 현재 시스템 구조와 문제점을 분석하고, 강화학습, 휴리스틱, 진화 알고리즘, 제약 기반 최적화 등 **가능한 개선 방안들을 비교 검토**한 뒤, 단계별 적용 계획을 제안합니다.

## 현재 시스템 구성

현재 메트릭 배치 작업은 **Apache Airflow**로 오케스트레이션되며, 세부 구성 및 자원 현황은 다음과 같습니다.

### Airflow DAG 및 파이프라인 단계

메트릭 측정 배치는 Airflow **DAG**로 정의된 **5단계 워크플로우**로 구성됩니다. 각 모듈에 대해 순차적으로 **`init → build → test → analysis → finish`** 단계가 실행되며, 단계별로 의존성이 설정된 Directed Acyclic Graph 형태로 구현되어 있습니다.

-   **init:** 측정을 위한 초기 설정 작업 (환경 준비 등)
-   **build:** 모듈 빌드 또는 필요한 컴파일 작업
-   **test:** 모듈에 대한 테스트 실행 및 메트릭 수집 (일부 모듈은 특수 플랫폼 필요)
-   **analysis:** 수집된 메트릭 데이터의 분석 및 정리
-   **finish:** 최종 결과 취합 및 완료 처리 (DB 저장 등)

Airflow 스케줄러는 각 모듈의 `init` 태스크가 완료되면 `build`를, 그 다음 `test`를 실행하는 식으로 **단계별 순차 진행**을 보장합니다. 전체적으로 DAG 구조가 **모듈별 병렬 실행**을 지원하되, 각 모듈 내부의 단계 순서는 위와 같이 고정되어 있습니다.

### CI/CD 인프라 자원 구성 (Jenkins 에이전트, 테스트 플랫폼)

실제 작업 실행은 **Jenkins** CI 시스템과 연동되어 이루어집니다. 현재 Jenkins에는 **총 32개의 에이전트 노드**가 등록되어 있어, 최대 32개의 태스크를 동시에 실행할 수 있는 병렬 처리 환경을 갖추고 있습니다. 이 에이전트들은 주로 `init`, `build`, `analysis`, `finish` 단계 작업을 분산 처리하는 데 사용됩니다. 한편 `test` 단계의 경우 일부 특정 모듈은 **전용 테스트 플랫폼**을 필요로 하는데, 이러한 **테스트 플랫폼은 1대**만 존재하여 **동시에 하나의 작업**만 처리할 수 있습니다. 즉, 대부분의 일반 테스트는 Jenkins 에이전트에서 병렬 수행 가능하지만, **특정 모듈의 테스트는 단일 장비에서 순차적으로 진행**해야 하는 **병목 구간**이 존재합니다.

### 테스트 샘플 및 모듈 규모

현재 배치 작업은 여러 프로젝트에 대해 수행되고 있으며, **프로젝트당 약 100개 내외의 모듈**을 포함합니다. 각 프로젝트는 **2~3개의 테스트 샘플** 구성을 갖고 있어, 하나의 프로젝트 내에서도 경우에 따라 모듈 별로 여러 번 테스트를 수행해야 합니다. 전체 모듈들 중 약 **10% 정도는 앞서 언급한 특수 테스트 플랫폼이 필요한 모듈**이며, 나머지 90%의 모듈 테스트는 일반 환경에서 실행 가능합니다. (즉, 병목 자원인 테스트 플랫폼에 **의존적인 작업이 전체의 10%** 수준입니다.) 이러한 규모로 인해, **동일한 시간 내 다수 모듈의 빌드/테스트를 처리**해야 하므로 효율적인 스케줄링이 필수적입니다.

### 모듈 실행 경로 및 데이터 수집

각 모듈은 Airflow DAG에 따라 정해진 경로로 실행되며, **Jenkins 에이전트 풀(32대)**과 **단일 테스트 플랫폼(1대)**을 이용해 위 단계를 모두 거칩니다. 예를 들어, 어떤 모듈은 Jenkins 에이전트에서 `build`를 완료한 후 대기하다가 테스트 플랫폼이 비는 즉시 `test`를 수행하고, 다시 에이전트를 할당받아 `analysis`를 진행하는 식입니다.
모든 단계의 **실행 시작/종료 시간과 소요시간 데이터는 InfluxDB**에 기록되고 있습니다. 즉, 각 모듈별로 `init`부터 `finish`까지 단계별 걸린 시간이 **시계열 데이터**로 저장되어 있어, **어느 단계에서 지연이 발생했는지 모니터링**할 수 있습니다. 이러한 데이터는 향후 스케줄링 최적화 알고리즘의 평가 및 학습 데이터로도 활용될 수 있습니다.

## 문제점 및 개선 목표

현재 운영 중인 배치 작업에는 다음과 같은 **비효율 문제**가 확인되고 있으며, 이를 개선하기 위한 목표가 정의됩니다:

-   **무작위 실행으로 인한 비효율:** 현재 각 모듈들의 실행 순서는 특별한 **최적화 없이 임의**로 정해지고 있어, 자원 사용이 비균형적이고 **불필요한 대기시간**이 발생합니다. 예를 들어, 실행 시간이 매우 긴 작업이 먼저 스케줄되면 짧은 작업들이 뒤에서 대기하게 되거나, 테스트 플랫폼이 한동안 놀다가 나중에 몰려서 과부하되는 등의 비효율이 있습니다. 이처럼 **비최적화된 스케줄링으로 전체 소요시간이 증가**하고 있습니다.

-   **자원 제약 하에서의 전체 실행시간 최소화:** 주어진 Jenkins 에이전트 32개와 테스트 플랫폼 1대라는 **유한한 자원 내에서 전체 작업 완료시간(makespan)을 최소화**해야 합니다. 즉, **병렬 처리**를 최대한 활용하면서도, **단일 자원(테스트 플랫폼)**에 의한 지연을 줄이는 스케줄링이 필요합니다. 목표는 현재 **N시간 소요되는 전체 파이프라인을 가능한 한 단축**하여, 일정 내 모든 메트릭 수집을 완료하고 여유를 확보하는 것입니다.

-   **운영 환경에 즉시 적용 가능해야 함:** 제안되는 개선안은 **바로 현업 시스템에 적용**할 수 있어야 합니다. 현재 별도의 시뮬레이션 환경이나 스테이징이 없어, **실제 운영 중인 환경에서 바로 테스트 및 적용**해야 합니다. 따라서 안전성이 낮거나 시행착오를 많이 요구하는 방법은 곧바로 적용하기 어려우며, **점진적인 검증과 단계별 도입**이 필요합니다. 또한 배치 작업은 계속 일일 단위로 돌아가야 하므로, **학습이나 최적화에 과도한 시간**을 쓰기 힘든 현실도 고려해야 합니다.

요약하면, **현행 무작위 스케줄링의 비효율**을 제거하고 **자원 제약을 고려한 최적의 작업 순서**를 결정함으로써, **전체 처리 시간을 단축**하는 것이 핵심 과제입니다. 이를 위해 다양한 AI 기반 기법들을 검토하고, **운영 환경에 맞는 현실적인 적용 방안**을 모색해야 합니다.

## AI 기반 개선 방안 비교

배치 작업 스케줄링 최적화를 위해 고려한 **AI/알고리즘 기반 접근법**에는 **① 강화학습**, **② 휴리스틱 규칙 기반**, **③ 진화 알고리즘**, **④ 수학적 제약 최적화**의 네 가지 범주가 있습니다. 각 방법의 **개념과 특징**을 간략히 설명하고, **장단점**, **운영 적용 용이성**, **예상 성능**을 비교합니다.

-   **강화학습 (Reinforcement Learning, RL):** 에이전트가 환경과 상호작용하며 **보상**을 최대화하는 방향으로 최적의 스케줄링 정책을 학습하는 방법입니다. 우리 사례에서는 작업들의 진행 상황(예: 남은 작업, 자원 상태 등)을 **상태(state)**로 두고, 어떤 작업을 다음에 실행할지 **행동(action)**으로 결정하며, **전체 완료시간을 단축하는 방향으로 보상(reward)**을 설계할 수 있습니다. 딥러닝을 활용한 **DQN (Deep Q-Network)** 같은 기법이나, **Graph Neural Network**로 DAG 구조를 표현한 모델을 사용해 복잡한 스케줄링 정책을 학습하는 방안이 있습니다 ([](https://staff.fnwi.uva.nl/a.d.pimentel/artemis/AIAI23.pdf#:~:text=suited%20for%20the%20task%20at,Path%02on)). **Offline RL**을 도입하면 과거 실행 로그 데이터만으로 모델을 훈련시켜, 운영 환경에서 **별도 시뮬레이션 없이** 학습을 진행할 수도 있습니다 ([[2410.15714] Offline reinforcement learning for job-shop scheduling problems](https://arxiv.org/abs/2410.15714#:~:text=issues%20and%20neglect%20of%20the,art%20techniques)).

-   **휴리스틱 방식:** 특정 우선순위 규칙에 따라 작업 실행 순서를 정하는 **규칙 기반 스케줄링**입니다. 구현이 간단하고 빠르게 적용할 수 있는 것이 장점이며, 오래 전부터 생산 스케줄링 등에 널리 쓰여온 기법입니다. 예를 들어 **SPT (Shortest Processing Time first, 처리시간 가장 짧은 작업 우선)**, **LPT (Longest Processing Time first, 처리시간 가장 긴 작업 우선)** 같은 규칙이 있으며, 또는 **임계 경로 우선**, **라운드로빈** 등 다양한 커스텀 규칙을 정의할 수 있습니다. 우리 시나리오에서는 **테스트 단계의 대기 최소화** 등을 목표로, 예컨대 *테스트 소요시간이 짧은 작업을 먼저 테스트*하는 규칙(SPT 류)이나 *빌드 완료 즉시 테스트 플랫폼이 놀지 않도록 대기 중 작업을 바로 투입*하는 규칙 등을 생각해볼 수 있습니다. 이러한 휴리스틱은 **단일 규칙으로는 항상 최적을 보장할 수 없지만**(모든 상황에 항상 우월한 단일 룰은 존재하지 않음 ([](https://staff.fnwi.uva.nl/a.d.pimentel/artemis/AIAI23.pdf#:~:text=size,strategy%20always%20outperforms%20all%20other))), **경험적**으로 성능이 좋고 구현 난이도가 매우 낮습니다.

-   **진화 알고리즘:** **유전 알고리즘 (GA)**, **시뮬레이티드 애닐링 (SA)** 등 자연 현상에 착안한 **메타휴리스틱 최적화 기법**입니다. 일정 길이의 스케줄(예: 작업 순서의 리스트)을 염색체로 보고 **유전자 교차와 변이 연산**을 통해 최적의 스케줄을 탐색하는 GA, 혹은 현재 스케줄을 조금씩 무작위로 변경하며 **서서히 탐색 범위를 좁혀가는** SA 기법 등을 활용할 수 있습니다. 이러한 방법들은 **복잡한 탐색 공간에서 준최적해를 찾는 데 유용**하며, 스케줄링 분야에도 오랫동안 활용되어 왔습니다 ([](https://arxiv.org/pdf/2106.04854#:~:text=Genetic%20algorithm%20is%20used%20very,10%5D.%20In)). 예를 들어, 어떤 연구에서는 CI 시스템의 작업 스케줄에 GA를 적용하여 **약 20%의 처리 시간 단축**을 이루었다는 보고도 있습니다 ([](https://arxiv.org/pdf/2106.04854#:~:text=of%20machines%20that%20jobs%20needed,as%20machine%20count%20and%20machine)). 진화 알고리즘은 특정 도메인 지식 없이도 적용 가능하고, 여러 제약을 포함한 문제에도 유연하게 대응할 수 있는 장점이 있습니다.

-   **제약 기반 모델 (CP/MILP 등):** 스케줄링 문제를 **수리 최적화 모델**로 수식화하여 푸는 접근입니다. 예를 들어 **혼합정수선형계획법 (MILP)**으로 각 작업의 시작/종료 시간을 변수로 두고, 자원 제약 (동시에 한 자원에 하나만 실행), 선행 관계 제약 (빌드 후 테스트) 등을 수식으로 표현한 뒤, **전체 완료시간을 최소화**하는 최적해를 구합니다. 혹은 **제약 프로그래밍 (CP)** 기법을 활용하여 OR-Tools와 같은 도구로 해를 찾을 수도 있습니다. 이 방식은 **해가 존재하면 반드시 찾고, 최적해에 수학적으로 수렴**하는 **엄밀한 접근법**입니다. 작은 규모의 문제에는 정확한 해법을 빠르게 찾을 수 있고, **추가적인 제약 (마감시간, 우선순위 등)도 손쉽게 모델에 반영**할 수 있습니다. 다만, 일반적으로 **Job-Shop 스케줄링 같은 NP-난이도 문제**에서 MILP/CP 접근법은 **태스크 수가 증가할수록 계산량이 폭증**하는 단점이 있습니다 ([](https://staff.fnwi.uva.nl/a.d.pimentel/artemis/AIAI23.pdf#:~:text=state%02spaces%2C%20as%20for%20example%20in,RL%29%2C%20however%2C%20may%20still)). 특히 작업 수가 많아지면 MILP로 최적해를 찾는 데 **실질적으로 불가능에 가까운 시간**이 소요될 수 있습니다 (예: 15개 작업 최적화에 24시간 초과, 100개 작업에 이론상 35년 소요 ([](https://staff.fnwi.uva.nl/a.d.pimentel/artemis/AIAI23.pdf#:~:text=state%02spaces%2C%20as%20for%20example%20in,RL%29%2C%20however%2C%20may%20still))). 따라서 실제로는 **제한된 시간 내에 찾아낸 최선의 해**를 사용하거나, 문제 규모를 줄이는 휴리스틱과 결합하여 사용하게 됩니다.

위 네 가지 접근법의 장단점을 종합적으로 비교하면 다음과 같습니다:

<table>
<tr>
  <th>접근 방법</th>
  <th>장점</th>
  <th>단점</th>
  <th>운영 적용 용이성</th>
  <th>성능 기대치</th>
</tr>
<tr>
  <td><b>강화학습 (RL)</b><br>(DQN, Offline RL 등)</td>
  <td>● 복잡한 스케줄 패턴을<br>학습을 통해 자동 발견<br>● Graph NN 등으로 DAG<br>구조를 활용 가능<br>● Offline RL로 실제 로그<br>데이터로 학습 가능</td>
  <td>● 충분한 훈련 데이터와<br>시간 필요 (학습 오래 걸림)<br>● 초기 구현 및 검증 복잡<br>● 정책 결과 해석 어려움<br>※ (심층 RL은 학습 속도가<br>느리고 일반화 어려움 ([[2410.15714] Offline reinforcement learning for job-shop scheduling problems](https://arxiv.org/abs/2410.15714#:~:text=solving%20combinatorial%20optimization%20problems%20in,a%20heterogeneous%20graph%20and%20the)))</td>
  <td>낮음<br>– 별도 모델 개발 필요,<br>운영 적용까지 장기간 소요<br>– 학습된 모델을 예측에만<br>사용하므로 실행시간 영향은 적음</td>
  <td>높음<br>– 충분히 학습되면<br>최적해 근접 기대 ([](https://staff.fnwi.uva.nl/a.d.pimentel/artemis/AIAI23.pdf#:~:text=suited%20for%20the%20task%20at,Path%02on))<br>– 복잡한 상황에서도<br>휴리스틱 상회 성능 가능</td>
</tr>
<tr>
  <td><b>휴리스틱<br>규칙 기반</b><br>(SPT, LPT 등)</td>
  <td>● 알고리즘이 단순하고<br>구현 용이 (즉시 적용 가능)<br>● 실행 오버헤드 거의 없음<br>● 직관적 이해 및 조정 가능</td>
  <td>● 복잡한 시나리오에서<br>한계 (항상 최적 아님 ([](https://staff.fnwi.uva.nl/a.d.pimentel/artemis/AIAI23.pdf#:~:text=size,strategy%20always%20outperforms%20all%20other)))<br>● 특정 상황에만 잘 맞고<br>범용성 부족<br>● 고정 룰이라 변화 대응 미흡</td>
  <td>매우 높음<br>– 간단한 코드 수정으로<br>바로 프로덕션 적용 가능<br>– 실패시 리스크 낮음 (원복 쉬움)</td>
  <td>낮음 ~ 중간<br>– 현재 무작위 대비<br>개선 효과 있으나<br>– 최적 대비 일정 차이<br>발생할 수 있음</td>
</tr>
<tr>
  <td><b>진화 알고리즘</b><br>(GA, SA 등)</td>
  <td>● 복잡한 최적화 문제에<br>대한 검증된 접근법 ([](https://arxiv.org/pdf/2106.04854#:~:text=Genetic%20algorithm%20is%20used%20very,10%5D.%20In))<br>● 전역 탐색으로 양질 해<br>발견 가능 (근사 최적)<br>● 제약 조건을 유연하게<br>포함 가능<br>※ (실제 CI 환경 적용시<br>약 20% 효율 개선 사례 ([](https://arxiv.org/pdf/2106.04854#:~:text=of%20machines%20that%20jobs%20needed,as%20machine%20count%20and%20machine)))</td>
  <td>● 탐색 과정에 많은<br>연산 (시간) 소모<br>● 해마다 결과가 달라<br>재현성 낮을 수 있음<br>● 해 찾는 시간이 제한되면<br>성능 저하 가능<br>● 실시간 동적 대응 어려움</td>
  <td>중간<br>– 오픈소스 라이브러리나<br>기존 구현 활용 가능<br>– 스케줄 계산에 수 초~분<br>소요될 수 있음 (모니터링 필요)</td>
  <td>중간 ~ 높음<br>– 제한 시간 내 최적해에<br>근접한 스케줄 기대<br>– 충분한 연산 시간 부여시<br>휴리스틱보다 우수</td>
</tr>
<tr>
  <td><b>제약 기반 최적화</b><br>(CP, MILP,<br>OR-Tools 등)</td>
  <td>● 최적해 산출 가능<br>(작은 문제의 경우)<br>● 해법에 대한 수학적<br>품질 보장 (검증 용이)<br>● 다양한 제약 정확히 반영<br>가능 (유연한 모델링)</td>
  <td>● 문제 규모 커지면<br>연산 불가 수준으로 급증 ([](https://staff.fnwi.uva.nl/a.d.pimentel/artemis/AIAI23.pdf#:~:text=state%02spaces%2C%20as%20for%20example%20in,RL%29%2C%20however%2C%20may%20still))<br>● 모델링을 위한 전문성<br>및 개발 노력 필요<br>● 동적 변화 대응 위해서는<br>재모델링/재계산 필요</td>
  <td>중간<br>– OR-Tools 등 활용시<br>비교적 구현 가능하나<br>– 매 배치마다 최적화 수행<br>필요 (시간 제한 설정 권장)</td>
  <td>높음 (작은 규모)<br>– 최적 솔루션 산출<br>중간 (큰 규모)<br>– 시간 내 찾은 근사해<br>– 휴리스틱 대비 우수하나<br>RL에 뒤처질 수 있음</td>
</tr>
</table>

**<참고>** 강화학습을 활용하면 DAG 스케줄링 문제에서 기존 휴리스틱 기법들(예: HEFT, CPOP 등)보다 **우수한 결과**를 낼 수 있다는 연구들이 보고되고 있으며 ([](https://staff.fnwi.uva.nl/a.d.pimentel/artemis/AIAI23.pdf#:~:text=problems%20such%20as%20the%20Travelling,Path%02on)), 반면 MILP 등의 **수리 최적화는 태스크 수가 조금만 늘어나도 계산 시간이 기하급수적으로 증가**하는 한계가 있습니다 ([](https://staff.fnwi.uva.nl/a.d.pimentel/artemis/AIAI23.pdf#:~:text=state%02spaces%2C%20as%20for%20example%20in,RL%29%2C%20however%2C%20may%20still)). 진화 알고리즘은 다양한 스케줄링 문제에 적용되어 왔지만 ([](https://arxiv.org/pdf/2106.04854#:~:text=Genetic%20algorithm%20is%20used%20very,10%5D.%20In)), 최적 해를 보장하지는 못하므로 보통 제한된 시간 내 **적정 수준의 해**를 찾는 용도로 활용됩니다. 휴리스틱 규칙은 **가장 손쉽게 적용**할 수 있으나 복잡도가 높아지면 성능상 한계가 있으므로, 필요하면 여러 규칙을 조합하거나 동적 우선순위 조정과 같은 보완이 요구될 수 있습니다.

## 향후 단계 제안

현 실환경에 새로운 스케줄링 기법을 적용하는 데 있어 **점진적 접근**이 바람직합니다. 위험을 최소화하면서 효과를 검증하기 위해 **단기 → 중기 → 장기** 단계별 로드맵을 제안합니다.

### 단기 계획: 휴리스틱/메타휴리스틱을 통한 빠른 개선

**즉각적인 성능 개선**을 위해, 현 단계에서는 구현이 용이한 **휴리스틱 또는 메타휴리스틱 기법을 우선 적용**합니다. 예를 들어, 테스트 단계의 대기시간을 줄이기 위해 **SPT 규칙**(테스트 소요시간이 짧은 모듈 우선)을 적용하거나, 빌드 완료된 작업을 즉시 테스트 큐에 넣는 간단한 스케줄링 정책을 Airflow 또는 Jenkins 레벨에서 구현합니다. 이러한 규칙 기반 개선은 개발 난이도가 낮고 바로 효과를 볼 수 있으며, 문제 발생 시에도 쉽게 원래 방식으로 **롤백**할 수 있어 안전합니다.
또한 약간 더 진보된 접근으로, **소규모 진화 알고리즘**(예: SA를 이용한 작업 순서 미세 조정)을 도입해 볼 수도 있습니다. 하루 배치 작업에 대한 스케줄을 시작 전에 **오프라인으로 계산**하여 적용하는 방식으로, 시스템에 큰 변경 없이도 일정 수준의 최적화를 달성할 수 있습니다. 단기적으로는 이렇게 **낮은 리스크의 기법들로 현재의 무작위 스케줄링보다 나은 결과**를 얻고, 그 효과(전체 실행시간 단축 정도, 자원 활용도 향상 등)를 InfluxDB 데이터로 모니터링합니다.

### 중기 계획: 과거 데이터 기반 Offline RL 준비

강화학습 기반의 **고도화된 최적화**를 위해 **사전 준비 단계**를 진행합니다. 우선 현재까지 축적된 **스케줄링 이력 데이터(InfluxDB의 단계별 시간 데이터)**를 활용하여, **환경 모델을 구성하거나 Offline RL 알고리즘의 학습 데이터**로 가공합니다. 구체적으로는, 각 배치의 작업 순서와 그에 따른 완료시간 결과를 **상태-행동-보상** 형태의 데이터로 변환하여 **Offline RL** 알고리즘(DQN 또는 정책 그래디언트 기법 등)에 투입할 준비를 합니다. 이때 우리 문제에 맞는 **상태 표현**(예: 대기 중인 작업들의 특성, 자원 점유 상황을 표현하는 벡터/그래프 등)과 **보상 함수**(makespan 단축에 비례하는 보상 등)를 설계합니다.
병렬로, **시뮬레이션 환경**을 간이로라도 구축하거나, 최소한 **Offline RL 학습을 검증할 테스트 베드**를 마련합니다 (예: 과거 며칠치 데이터를 활용한 에피소드 생성). 중기 목표는, **실제 데이터를 통해 학습된 RL 모델**을 얻는 것입니다. 이 모델 자체는 아직 운영에 투입하지 않고, 과거 배치들의 스케줄링 결과와 비교하여 **시뮬레이션 평가**를 진행합니다. 만약 Offline RL의 학습이 순조롭고 충분한 성능 개선을 보여준다면, 다음 단계로 넘어갈 준비가 된 것입니다. 이 기간 동안 계속 **데이터를 축적**하고 알고리즘을 개선하여, **안정적이고 일반화 능력이 있는 정책**을 얻는 데 집중합니다.

### 장기 계획: 학습된 정책의 실시간 적용 및 지속적 개선

최종 단계에서는, **학습된 AI 스케줄링 정책을 실제 운영 환경에 통합**합니다. 중기 단계에서 충분한 성능과 안정성이 입증된 RL 모델 (또는 기타 최적화 모델)을 Airflow/Jenkins 스케줄러에 적용하여, **실시간으로 작업 순서를 결정**하도록 합니다. 예를 들어, Airflow의 태스크 스케줄링 로직을 커스터마이징하거나, 별도 서비스로 **스케줄러 에이전트**를 두어 현재 대기 중인 작업들에 대해 RL 에이전트가 **다음 실행할 작업을 추천**하도록 할 수 있습니다. 운영 적용 초반에는 보수적으로, **기존 휴리스틱 방식과 AI 방식의 결과를 비교하는 A/B 테스트** 또는 **섀도우 테스트**를 거쳐 안정성을 확인합니다. 문제없다고 판단되면 AI 스케줄러를 기본으로 채택하여 운영을 최적화합니다.
장기적으로는 **지속적인 개선 사이클**을 구축합니다. 운영 중 새롭게 발생하는 데이터(예: 모듈 추가, 환경 변화 등)를 주기적으로 수집하고 Offline RL로 **정책을 재훈련/업데이트**하여, **환경 변화에 적응**합니다. 또한 Extreme한 상황(예: 특정 날에 빌드 실패로 일부 모듈 재시도 등)에 대한 대응도 강화학습의 보상 설계를 조정하거나 규칙을 보완하여 반영합니다. 이렇게 함으로써 스케줄링 정책이 **점진적으로 자기 학습**을 통해 향상되고, 운영 효율도 꾸준히 개선될 것입니다.

**지속 개선 체계:** 운영 적용 후에도 모니터링을 통해 목표 지표(전체 실행시간, 에이전트 가동률, 테스트 플랫폼 대기시간 등)를 추적합니다. 만약 성능이 기대에 못 미치거나 특정 모듈에서 이상 현상이 발견되면, 원인을 분석하여 **정책을 튜닝**합니다. DevOps 팀 내에 정기적으로 성능 리포트를 공유하고, 필요시 알고리즘 파라미터(예: GA 세대 수, RL 탐험계수 등)를 조정하거나 모델을 재훈련합니다. 궁극적으로는 이러한 AI 기반 스케줄링이 **안정적으로 정착**되어, 현재의 메트릭 측정 배치 작업을 **최소한의 시간과 자원으로 달성**하도록 만드는 것이 장기 목표입니다.

以上の단계를 통해, 위험을 관리하면서도 최신 AI 기법을 순차적으로 도입하여 **배치 작업 스케줄링 최적화**를 달성할 수 있을 것으로 기대합니다. 각 단계의 성과는 팀 내에 공유하여 피드백을 반영하고, 성공적인 사례를 축적함으로써 향후 다른 파이프라인에도 AI 최적화 적용을 확산할 수 있을 것입니다.
